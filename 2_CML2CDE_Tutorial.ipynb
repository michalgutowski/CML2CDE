{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b4f32e-7293-4774-9a5a-402811297ebe",
   "metadata": {},
   "source": [
    "## CML 2 CDE Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2601e88-4ea3-4445-b499-608efead611d",
   "metadata": {},
   "source": [
    "##### CDP users can develop Spark Jobs interactively in CML without losing the benefits of CDE, the Spark purpose-built Data Service in the Cloudera Data Platform\n",
    "\n",
    "* CDE offers a rich CLI and API. This is normally used to manage workflows with CI/CD tools \n",
    "* The CLI and API can be used within a CML Session\n",
    "* You can automate deployments to CDE and version project artifact with CML's API V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9903262-b2a1-400b-8ee3-54eafb9721fb",
   "metadata": {},
   "source": [
    "### Part 1: A simple PySpark Job in CML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7330fa-eb6b-4ecb-9bd7-aeb84a926d55",
   "metadata": {},
   "source": [
    "#### You can prototype your PySpark (or Scala Spark) Jobs interactively with CML Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965d550c-15b1-406e-a70a-ba92f71b60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "import sys\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd0548e-6258-48ab-abbb-42f461e65de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing schema\n",
      "root\n",
      " |-- LoanAmount: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zip: integer (nullable = true)\n",
      " |-- NAICSCode: integer (nullable = true)\n",
      " |-- BusinessType: string (nullable = true)\n",
      " |-- RaceEthnicity: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Veteran: string (nullable = true)\n",
      " |-- NonProfit: string (nullable = true)\n",
      " |-- JobsRetained: integer (nullable = true)\n",
      " |-- DateApproved: string (nullable = true)\n",
      " |-- Lender: string (nullable = true)\n",
      " |-- CD: string (nullable = true)\n",
      "\n",
      "How many TX records did we get?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got: 337237 \n",
      "Creating TexasPPP Database \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = d577966e-7c97-41eb-8f54-9895a15d107a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        databaseName|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|             airline|\n",
      "|          airline_dw|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|       airlines_csv1|\n",
      "|   airlines_csv_vish|\n",
      "|    airlines_iceberg|\n",
      "|   airlines_iceberg1|\n",
      "|airlines_iceberg_...|\n",
      "|          airquality|\n",
      "|              apache|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|              bhagan|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|            climrisk|\n",
      "|                cnav|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Inserting Data into TexasPPP.loan_data table \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|RecordCount|\n",
      "+-----------+\n",
      "|    1011711|\n",
      "+-----------+\n",
      "\n",
      "Retrieve 15 records for validation \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+-----+--------------------+---------+------------+------------+--------------------+\n",
      "|LoanAmount|         City|State|  Zip|        BusinessType|NonProfit|JobsRetained|DateApproved|              Lender|\n",
      "+----------+-------------+-----+-----+--------------------+---------+------------+------------+--------------------+\n",
      "|   20000.0|        ALVIN|   TX|77511| Sole Proprietorship|     null|          11|  04/06/2020|Texas Advantage C...|\n",
      "|   20000.0|       HEARNE|   TX|77859|Limited  Liabilit...|     null|           1|  04/30/2020|First National Ba...|\n",
      "|   20000.0|     CLEBURNE|   TX|76033|Subchapter S Corp...|     null|        null|  04/30/2020|Commonwealth Busi...|\n",
      "|   20000.0|      HOUSTON|   TX|77006|Subchapter S Corp...|     null|           4|  04/14/2020|     Allegiance Bank|\n",
      "|   20000.0|   ROPESVILLE|   TX|79358|         Corporation|     null|           4|  04/06/2020|          Vista Bank|\n",
      "|   20000.0|      HOUSTON|   TX|77080| Sole Proprietorship|     null|           1|  06/30/2020|Amerant Bank, Nat...|\n",
      "|   20000.0|         WACO|   TX|76701|Limited  Liabilit...|     null|           1|  04/28/2020|American Bank, Na...|\n",
      "|   20000.0|    SPLENDORA|   TX|77372| Sole Proprietorship|     null|           2|  06/30/2020|      Customers Bank|\n",
      "|   20000.0|MISSOURI CITY|   TX|77459|Subchapter S Corp...|     null|           3|  06/30/2020|MBE Capital Partners|\n",
      "|   20000.0| CRYSTAL CITY|   TX|78839|Limited  Liabilit...|     null|           7|  04/14/2020|Lone Star Nationa...|\n",
      "|   20000.0|       SPRING|   TX|77381| Sole Proprietorship|     null|           2|  06/30/2020|      Customers Bank|\n",
      "|   20000.0|        PLANO|   TX|75023|Limited  Liabilit...|     null|          13|  04/14/2020|          Hanmi Bank|\n",
      "|   20000.0|       AUSTIN|   TX|78735|         Corporation|     null|           1|  05/03/2020|Bank of America, ...|\n",
      "|   20000.0|     ROCKWALL|   TX|75032|Limited  Liabilit...|     null|           1|  06/30/2020|    Texas First Bank|\n",
      "|   20000.0| FLOWER MOUND|   TX|75028|         Corporation|     null|           1|  05/03/2020|Bank of America, ...|\n",
      "+----------+-------------+-----+-----+--------------------+---------+------------+------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark PPP ETL\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", os.environ[\"REGION\"])\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ[\"STORAGE\"])\\\n",
    "    .getOrCreate()  \n",
    "    \n",
    "#Path of our file in S3\n",
    "input_path = os.environ[\"STORAGE\"] + '/datalake/cde-demo/PPP-Sub-150k-TX.csv'\n",
    "\n",
    "#This is to deal with tables existing before running this code. Not needed if you're starting fresh.\n",
    "#spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "\n",
    "#Bring data into Spark from S3 Bucket\n",
    "base_df=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(input_path)\n",
    "#Print schema so we can see what we're working with\n",
    "print(f\"printing schema\")\n",
    "base_df.printSchema()\n",
    "\n",
    "#Filter out only the columns we actually care about\n",
    "filtered_df = base_df.select(\"LoanAmount\", \"City\", \"State\", \"Zip\", \"BusinessType\", \"NonProfit\", \"JobsRetained\", \"DateApproved\", \"Lender\")\n",
    "\n",
    "#This is a Texas only dataset but lets do a quick count to feel good about it\n",
    "print(f\"How many TX records did we get?\")\n",
    "tx_cnt = filtered_df.count()\n",
    "print(f\"We got: %i \" % tx_cnt)\n",
    "\n",
    "#Create the database if it doesnt exist\n",
    "print(f\"Creating TexasPPP Database \\n\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS TexasPPP\")\n",
    "spark.sql(\"SHOW databases\").show()\n",
    "\n",
    "print(f\"Inserting Data into TexasPPP.loan_data table \\n\")\n",
    "\n",
    "#insert the data\n",
    "filtered_df.\\\n",
    "  write.\\\n",
    "  mode(\"append\").\\\n",
    "  saveAsTable(\"TexasPPP\"+'.'+\"loan_data\", format=\"parquet\")\n",
    "\n",
    "#Another sanity check to make sure we inserted the right amount of data\n",
    "print(f\"Number of records \\n\")\n",
    "spark.sql(\"Select count(*) as RecordCount from TexasPPP.loan_data\").show()\n",
    "\n",
    "print(f\"Retrieve 15 records for validation \\n\")\n",
    "spark.sql(\"Select * from TexasPPP.loan_data limit 15\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161701a6-1b94-4f72-ba58-153ac1bcea6b",
   "metadata": {},
   "source": [
    "### Part 2: Working with the CDE CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4f4bc-2b23-46fd-a087-acfa14a49cf2",
   "metadata": {},
   "source": [
    "#### You can submit the same PySpark Job into your CDE Virtual Cluster with the CDE CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe9e01-faae-42ae-a86a-44d7a5b31d81",
   "metadata": {},
   "source": [
    "#### Open the CML Session Terminal and execute the following Spark Submit. You will need to enter your CDP Workload Password"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a22c88-bc79-45db-922b-6790dee3ce99",
   "metadata": {},
   "source": [
    "<code> cde spark submit /home/cdsw/Data_Extraction_Sub_150k.py </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20aef7e-664a-49dd-9953-35f0dd2acb5b",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080f2a0-efa3-4229-9948-f89bbf251583",
   "metadata": {},
   "source": [
    "#### Navigate to your CDE Virtual Cluster's Job Runs page and validate that the job has been created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e369b8-1c08-47a9-8a36-5e7e3d59a92f",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6f25c-1368-43b4-89f7-3b03a6f5b460",
   "metadata": {},
   "source": [
    "#### Important Takeaways\n",
    "\n",
    "* We pasted the same PySpark code we executed in the previous cells in \"/home/cdsw/Data_Extraction_Sub_150k.py\". No code changes required.\n",
    "* The CLI has been previously installed. This is tied to a specific CDE Virtual Cluster as when we installed it we had to set the VC JOBS API URL.\n",
    "* Notice that althouigh the Spark Job is running as a CDE Job, CDE does not create a reusable CDE Job for it. You can only rerun by resubmitting the Spark Submit above.\n",
    "* However, you can use the CDE CLI to create CDE Jobs, Resources, and more. This is better for monitoring and troubleshooting the same Spark Job across multiple runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ffb67-ea71-4cc9-babc-8c92aec61c90",
   "metadata": {},
   "source": [
    "#### Learning to Use the CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fa220-498e-44fa-940a-5e631684843d",
   "metadata": {},
   "source": [
    "#### For a full reference to the CLI, please visit this site: https://docs.cloudera.com/data-engineering/cloud/cli-access/topics/cde-cli-reference.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bfff4-54a6-440d-bc3a-e26c48babc08",
   "metadata": {},
   "source": [
    "#### Once you identified a CLI command you want to execute, you can use the terminal to familiarize yourself with all flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e235951-f024-4682-9aac-afae2d388bf1",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2fd74c-5714-4945-a306-609ce66a52d8",
   "metadata": {},
   "source": [
    "### More CDE CLI Command examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84200cdb-54bb-4cf3-8351-4964e62097db",
   "metadata": {},
   "source": [
    "##### Using the CML Session Terminal, execute the following in this order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7227926-2b6d-4fdf-8e72-27083d0ed25a",
   "metadata": {},
   "source": [
    "#### Create a CDE Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75e4d6-96a9-46f6-9687-0020cc79b015",
   "metadata": {},
   "source": [
    "##### A CDE Resource allows you to save and reuse CDE Job related artifacts  \n",
    "<code>cde resource create --name cml2cde_cli_resource </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b189273-f241-46de-871b-31cfd95390b9",
   "metadata": {},
   "source": [
    "#### Upload a File to a Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73beffa2-8b44-4b67-996d-000a21a345e8",
   "metadata": {},
   "source": [
    "<code>cde resource upload --name cml2cde_cli_resource --local-path \"/home/cdsw/Data_Extraction_Sub_150k.py\" --resource-path \"Data_Extraction_Sub_150k.py\"</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180c45e-5d22-4800-af81-0f824357e1b9",
   "metadata": {},
   "source": [
    "#### Create a CDE Job with the Resource File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d913f-a405-4fc8-ba21-76ee45932568",
   "metadata": {},
   "source": [
    "##### The Job command has a lot options. For example, you can create a CDE Job with the CDE Resource file and run it on a schedule. \n",
    "<code>cde job create --name \"cml2cde_cli_job\" --type \"spark\"\n",
    "                --application-file \"Data_Extraction_Sub_150k.py\" \n",
    "                --cron-expression \"0 */1 * * *\" \\\n",
    "                --schedule-enabled \"true\" \n",
    "                --schedule-start \"2022-04-29\" \n",
    "                --schedule-end \"2022-05-02\" \n",
    "                --mount-1-resource \"cml2cde_cli_resource\"</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b1f28-ff6a-4f12-aa40-0e8e837f8ec6",
   "metadata": {},
   "source": [
    "#### Search for CDE Jobs based on attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ef7b5-7da7-4bcd-8613-663ad9fedd09",
   "metadata": {},
   "source": [
    "##### You can use attributes for your search. In this case, we search by name.\n",
    "<code>cde job list --filter 'name[like]%cml2cde%'<code/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3cf6a-2d62-4563-a519-ef0a5ab342ef",
   "metadata": {},
   "source": [
    "#### List all CDE Job Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51206378-c0f9-4994-8db1-049c85eed065",
   "metadata": {},
   "source": [
    "##### A CDE Resource allows you to save and reuse CDE Job related artifacts  \n",
    "<code>cde run list</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc3e94-c06a-48ff-80f5-2d0ab718eb2e",
   "metadata": {},
   "source": [
    "#### Describe CDE Job Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560e156-52ec-4333-9a88-a7fbaa3aea0e",
   "metadata": {},
   "source": [
    "#### Replace the integer with your job run id e.g. 47 below\n",
    "<code>cde run describe --id 47</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5a99a-cf90-4567-ae53-753c7f803544",
   "metadata": {},
   "source": [
    "#### Create a CDE Job with Custom Spark Log Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbce3d-8788-413a-95b7-ed0759761953",
   "metadata": {},
   "source": [
    "#### Using the log-level parameter, you can choose any of the folling: (TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF)\n",
    "<code>cde job create --name \"cml2cde_cli_job_custom_log_level\" --type \"spark\"\n",
    "                --application-file \"Data_Extraction_Sub_150k.py\"\n",
    "                --log-level \"DEBUG\"\n",
    "                --schedule-enabled \"false\" \n",
    "                --mount-1-resource \"cml2cde_cli_resource\"</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ee3c0-7c2a-4975-b16f-72a7475f5dde",
   "metadata": {},
   "source": [
    "#### Trigger execution of the \"cml2cde_cli_job_custom_log_level\" Job:\n",
    "<code>cde job run --name \"cml2cde_cli_job_custom_log_level\" --application-file \"Data_Extraction_Sub_150k.py\"</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656445f-22a9-4a95-ac58-5b5f3c34c557",
   "metadata": {},
   "source": [
    "#### Navigate back to the corresponding CDE Job Run, pick a log type of preference and validate that the log type as now changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0b6da-c708-4e29-b4cb-334f3a0e4262",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e863e-616c-4b71-abf4-02f645614277",
   "metadata": {},
   "source": [
    "#### Collect CDE Job Run Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024837c-7ab8-498a-8602-dfcd769fc795",
   "metadata": {},
   "source": [
    "##### You can download the Spark Logs you would have access to in CDE. Notice you have more options e.g. executor logs\n",
    "<code>cde run logs --type \"driver/stdout\" --id 47<code/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84cbd0-2128-4b25-9b9d-eb04c4a69255",
   "metadata": {},
   "source": [
    "#### You can modify the log type to any of the available tabs in the corresponding CDE Job Run page. For example:\n",
    "* <code>\"driver/stderr\" or \"Driver/stdout\"<code/>\n",
    "* <code>\"executor id/stdout\"<code/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f7089-ab1a-42ad-9ebe-e1fc16867f3d",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a98e67-f6cc-470c-837a-64d08d3eb3d2",
   "metadata": {},
   "source": [
    "### Part 3: Working with the CDE API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba61b96-899c-4814-8062-55cf8f8704d9",
   "metadata": {},
   "source": [
    "##### In order to run the following commands you need to have set the JOBS_API_URL environment variables. Please ensure you have followed instructions located in the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ca58e-a18d-4439-aaab-1a6b91eb5639",
   "metadata": {},
   "source": [
    "#### You can submit the same PySpark Job into your CDE Virtual Cluster with the CDE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f993f33c-1b50-4927-a47f-8f23e56bd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a2fe4c2-27fd-477b-a6e7-aa9ffd051530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cde_token():\n",
    "    rep = os.environ[\"JOBS_API_URL\"].split(\"/\")[2].split(\".\")[0]\n",
    "    os.environ[\"GET_TOKEN_URL\"] = os.environ[\"JOBS_API_URL\"].replace(rep, \"service\").replace(\"dex/api/v1\", \"gateway/authtkn/knoxtoken/api/v1/token\")\n",
    "    token_json = !curl -u $WORKLOAD_USER:$WORKLOAD_PASSWORD $GET_TOKEN_URL\n",
    "    os.environ[\"ACCESS_TOKEN\"] = json.loads(token_json[5])[\"access_token\"]\n",
    "    return json.loads(token_json[5])[\"access_token\"]\n",
    "\n",
    "tok = set_cde_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba429ef8-6a9d-4506-be9d-ec06001d21df",
   "metadata": {},
   "source": [
    "#### You can perform the same commands you executed above with the CLI. You can execute each command from the terminal, or in a notebook cell with the \"!\" prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8344f6-0f95-4e03-9d1f-ae3936577871",
   "metadata": {},
   "source": [
    "#### Submit a request to create a CDE Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2860ad5-87db-4613-ac6c-1fb8473f0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CDE Resource with the API\n",
    "!curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X POST \\\n",
    "  \"$JOBS_API_URL/resources\" -H \"Content-Type: application/json\" \\\n",
    "  -d \"{ \\\"name\\\": \\\"cml2cde_api_resource\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67f984-3443-4114-b44b-57eab7f8825d",
   "metadata": {},
   "source": [
    "#### Validate that the resource has been created in your CDE Virtual Cluster by visiting the Resources tab again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dcaeb-e3ca-4c01-8cef-9d4a670da89b",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39efcea-cc90-4b4c-b3cf-099a40516d78",
   "metadata": {},
   "source": [
    "#### Check that the CDE resource has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7bdc89-a453-495e-a2da-88761fb7cfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"cml2cde_api_resource\",\"type\":\"files\",\"created\":\"2022-09-01T20:26:38Z\",\"modified\":\"2022-09-01T20:26:38Z\",\"lastUsed\":\"0001-01-01T00:00:00Z\",\"retentionPolicy\":\"keep_indefinitely\",\"status\":\"ready\"}"
     ]
    }
   ],
   "source": [
    "!curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X GET \"$JOBS_API_URL/resources/cml2cde_api_resource\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3740bd-713b-4dd0-9cee-6f94f155e0e7",
   "metadata": {},
   "source": [
    "#### Upload the Spark Job to the CDE Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ae8138-42c2-4de7-99a1-21c4bfd7a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X PUT \\\n",
    "  \"$JOBS_API_URL/resources/cml2cde_api_resource/Data_Extraction_Sub_150k.py\" \\\n",
    "  -F \"file=@/home/cdsw/cml2cde_tutorial_code/Data_Extraction_Sub_150k.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb52d74-bf82-44db-adcc-e769b9a7fe63",
   "metadata": {},
   "source": [
    "#### Create a CDE Job with the Uploaded File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b139db93-deb4-40bd-b954-7da263b06dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CDE Job from the resource\n",
    "!curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X POST \"$JOBS_API_URL/jobs\" \\\n",
    "          -H \"accept: application/json\" \\\n",
    "          -H \"Content-Type: application/json\" \\\n",
    "          -d \"{ \\\"name\\\": \\\"cml2cde_api_job\\\", \\\"type\\\": \\\"spark\\\", \\\"retentionPolicy\\\": \\\"keep_indefinitely\\\", \\\"mounts\\\": [ { \\\"dirPrefix\\\": \\\"/\\\", \\\"resourceName\\\": \\\"cml2cde_api_resource\\\" } ], \\\"spark\\\": { \\\"file\\\": \\\"Data_Extraction_Sub_150k.py\\\"},\\\"schedule\\\": { \\\"enabled\\\": false} }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f267e-c29e-4c15-b8ba-aa769d3396ce",
   "metadata": {},
   "source": [
    "#### Execute the CDE Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ae70ec-8852-45ab-9a48-c549a15b5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":48}"
     ]
    }
   ],
   "source": [
    "!curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X POST \"$JOBS_API_URL/jobs/cml2cde_api_job/run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea76361-3c4b-4e90-90a6-17b2bc85116f",
   "metadata": {},
   "source": [
    "#### As with the CDE CLI, the CDE API allows you to do a lot more than the above. \n",
    "#### The best way to start is by visiting the Swagger page for by going to the CDE Virtual Cluster Details Job, then opening the API DOC link as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d2bd9-fb2c-4b24-9e5a-d91677df1a54",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c7a8e-56bc-4fda-abc1-b985bedba206",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad56c6-3701-43db-a794-7f806e16f85e",
   "metadata": {},
   "source": [
    "#### You can build API commands for your cluster from there. For example, click on the “GET/jobs” method under the “jobs” section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cd8f5-87c0-4e30-87f8-0e0d7c6e9f8c",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9adfc-2d59-482e-a5a8-9df9bea2a6b2",
   "metadata": {},
   "source": [
    "#### Click on the “Try it out” button. \n",
    "\n",
    "#### Next, try entering a few options in the provided fields. For example, ensure the latestjob flag is set to \"true\" and enter “name[eq]cml2cde_api_job” string in the first field.\n",
    "\n",
    "#### Finally hit \"Execute\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cb3e7-b305-42be-be7b-d0aa4c69039c",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ece63b-8786-4210-bf03-60e64ddb6f8a",
   "metadata": {},
   "source": [
    "#### Notice the \"Request URL\" field has been populated for you. You can use this to construct your next request, as shown below. Also notice a response preview is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51354d-eb2f-4d6a-8b42-a2f6bb91632a",
   "metadata": {},
   "source": [
    "![alt text](images/cml2cde_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f151e-184c-4452-a11b-08098ffe5ec0",
   "metadata": {},
   "source": [
    "#### Using the highlighted portion of the \"Request URL\" we can construct a new request as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d669aeb8-d4ab-4d48-a71e-87d7fe14bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CDE Job from the resource\n",
    "latest_job_json = !curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X GET \"$JOBS_API_URL/jobs?latestjob=true&filter=name%5Beq%5Dcml2cde_api_job&limit=20&offset=0&orderby=name&orderasc=true\" \\\n",
    "          -H \"accept: application/json\" \\\n",
    "          -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c829d2-19e1-4105-8414-94ad2a43d6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobs': [{'name': 'cml2cde_api_job',\n",
       "   'type': 'spark',\n",
       "   'created': '2022-09-01T20:28:32Z',\n",
       "   'modified': '2022-09-01T20:28:32Z',\n",
       "   'lastUsed': '2022-09-01T20:28:39Z',\n",
       "   'mounts': [{'resourceName': 'cml2cde_api_resource', 'dirPrefix': '/'}],\n",
       "   'spark': {'file': 'Data_Extraction_Sub_150k.py'},\n",
       "   'retentionPolicy': 'keep_indefinitely',\n",
       "   'schedule': {'enabled': False, 'user': 'pauldefusco'},\n",
       "   'latestRunInfo': {'id': 48,\n",
       "    'job': 'cml2cde_api_job',\n",
       "    'type': 'spark',\n",
       "    'status': 'starting',\n",
       "    'mounts': [{'resourceName': 'cml2cde_api_resource', 'dirPrefix': '/'}],\n",
       "    'spark': {'spec': {'file': 'Data_Extraction_Sub_150k.py'},\n",
       "     'submitID': '22'},\n",
       "    'user': 'pauldefusco',\n",
       "    'started': '2022-09-01T20:28:39Z',\n",
       "    'ended': '0001-01-01T00:00:00Z',\n",
       "    'identity': {'disableRoleProxy': True, 'role': 'instance'}}}],\n",
       " 'meta': {'hasNext': False, 'limit': 20, 'offset': 0, 'count': 1}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(latest_job_json[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7f4ce-57d3-495f-96d7-944fdb0da331",
   "metadata": {},
   "source": [
    "### Part 4: Working with CDE in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4138c12-7efa-48c2-b5a0-63aca7af3ff3",
   "metadata": {},
   "source": [
    "#### You can reconstruct your CDE API requests with the Python requests library. Notice you will need a CDE Token, as you did with the CDE API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3904632f-4cfc-45f7-94cb-c14f0e4c00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cde_token():\n",
    "    rep = os.environ[\"JOBS_API_URL\"].split(\"/\")[2].split(\".\")[0]\n",
    "    os.environ[\"GET_TOKEN_URL\"] = os.environ[\"JOBS_API_URL\"].replace(rep, \"service\").replace(\"dex/api/v1\", \"gateway/authtkn/knoxtoken/api/v1/token\")\n",
    "    token_json = !curl -u $WORKLOAD_USER:$WORKLOAD_PASSWORD $GET_TOKEN_URL\n",
    "    os.environ[\"ACCESS_TOKEN\"] = json.loads(token_json[5])[\"access_token\"]\n",
    "    return json.loads(token_json[5])[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9724fe-6383-4328-b1e6-e1e3386ac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = set_cde_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee6939-80de-4005-895b-4c2e6a650d56",
   "metadata": {},
   "source": [
    "#### Browse Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce3c913-604b-47a5-a481-714fbdb148d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3380ad-e2d8-4a2b-bdfa-361b12e2cfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'FirstDag',\n",
       "  'type': 'files',\n",
       "  'signature': '70b62c9689bf594137762fa855e8ee434c95ead6',\n",
       "  'created': '2022-08-31T20:32:16Z',\n",
       "  'modified': '2022-09-01T00:21:33Z',\n",
       "  'lastUsed': '2022-09-01T00:21:58Z',\n",
       "  'retentionPolicy': 'keep_indefinitely',\n",
       "  'status': 'ready'},\n",
       " {'name': 'FlightsProcessIcebergJAR',\n",
       "  'type': 'files',\n",
       "  'signature': 'c6dbf07fa5925cb5778200391d0a987dc45d1d2d',\n",
       "  'created': '2022-07-28T21:13:37Z',\n",
       "  'modified': '2022-07-28T21:13:37Z',\n",
       "  'lastUsed': '2022-07-28T21:14:36Z',\n",
       "  'retentionPolicy': 'keep_indefinitely',\n",
       "  'status': 'ready'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = os.environ[\"JOBS_API_URL\"] + \"/resources\"\n",
    "myobj = {\"name\": \"cml2cde_python\"}\n",
    "headers = {\"Authorization\": f'Bearer {tok}', \n",
    "          \"Content-Type\": \"application/json\"}\n",
    "\n",
    "## Only showing the latest two resources\n",
    "x = requests.get(url, headers=headers)\n",
    "x.json()[\"resources\"][-3:-1]\n",
    "\n",
    "# CDE API Equivalent\n",
    "#!curl -H \"Authorization: Bearer ${CDE_TOKEN}\" -X GET \\\n",
    "#   \"${CDE_JOB_URL_AWS}/resources\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630501a8-60dd-4727-9971-b7be1503fd0a",
   "metadata": {},
   "source": [
    "#### Create a Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24738272-e033-4235-8057-4d30958fe471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "url = os.environ[\"JOBS_API_URL\"] + \"/resources\"\n",
    "myobj = {\"name\": \"cml2cde_python_resource\"}\n",
    "data_to_send = json.dumps(myobj).encode(\"utf-8\")\n",
    "headers = {\"Authorization\": f'Bearer {tok}', \n",
    "          \"Content-Type\": \"application/json\"}\n",
    "\n",
    "x = requests.post(url, data=data_to_send, headers=headers)\n",
    "print(x.status_code)\n",
    "\n",
    "# CDE API Equivalent\n",
    "#!curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -X POST \\\n",
    "#  \"$CDE_VC_ENDPOINT/resources\" -H \"Content-Type: application/json\" \\\n",
    "#  -d \"{ \\\"name\\\": \\\"cml2cde_resource\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bebdb7-39b6-450d-8bcd-702b1e86ba72",
   "metadata": {},
   "source": [
    "#### You can build more Python Requests with the help of the CDE API Documentation as you did in Part 3.\n",
    "\n",
    "#### If used in conjunction with CML API V2, this is probably the easiest way to build CDE Pipelines in CML. We will see an example in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2df28-6445-4858-853b-095a45ced3b3",
   "metadata": {},
   "source": [
    "### Part 5: Closing the Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6046b-e5cd-4dad-979d-d8c430b2700e",
   "metadata": {},
   "source": [
    "#### While you can submit Spark Jobs in any of the ways above, it's important to keep in mind that the outputs of CDE Jobs can be retrieved from CML just as easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10413e-1b57-490b-89e6-49ddd2c76e6c",
   "metadata": {},
   "source": [
    "#### To prove this, execute the \"Validation\" PySpark Job with the CDE CLI. The python file is in the home directory. For your convenience, the code is below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3102c3e-c3e8-4b57-b640-3c2f5ba0374f",
   "metadata": {},
   "source": [
    "#### Execute the Spark Submit from the CML Session terminal. \n",
    "<code>cde spark submit /home/cdsw/Validation.py<code/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed7cd4b9-28fe-4839-9470-5e735f266e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve 1000 records for validation \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "import sys\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark PPP ETL\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", \"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", \"s3a://demo-aws-go02\")\\\n",
    "    .getOrCreate()  \n",
    "    \n",
    "print(f\"Retrieve 1000 records for validation \\n\")\n",
    "df = spark.sql(\"Select * from TexasPPP.loan_data limit 1000\")\n",
    "\n",
    "df.\\\n",
    "  write.\\\n",
    "  mode(\"append\").\\\n",
    "  saveAsTable(\"ValidationTable\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e3587-4007-4bbb-97e7-c123eb0ac001",
   "metadata": {},
   "source": [
    "#### Next, execute the following cell and observe that the data is readily available in CML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b99f29e8-27ee-439d-95a3-3a5c7d066732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### You may have lost your Spark Session by now. If that is the case, recreate one:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "import sys\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark PPP ETL\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", os.environ[\"REGION\"])\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ[\"STORAGE\"])\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7d9feca-26af-44ee-8ad1-ce66c080387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve Data from the New table \n",
      "\n",
      "+----------+--------------+-----+-----+--------------------+---------+------------+------------+--------------------+\n",
      "|LoanAmount|          City|State|  Zip|        BusinessType|NonProfit|JobsRetained|DateApproved|              Lender|\n",
      "+----------+--------------+-----+-----+--------------------+---------+------------+------------+--------------------+\n",
      "| 149997.07|   COLLEYVILLE|   TX|76034|         Corporation|     null|          14|  04/28/2020|    Bank of the West|\n",
      "|  149990.0|        LAMESA|   TX|79331|Limited  Liabilit...|     null|          12|  04/13/2020|   First United Bank|\n",
      "|  149965.0|CORPUS CHRISTI|   TX|78418|         Corporation|     null|           1|  04/15/2020|American Bank, Na...|\n",
      "|  149956.0|        AUSTIN|   TX|78704|         Corporation|     null|          11|  04/10/2020|  PlainsCapital Bank|\n",
      "|  149952.0|     KERRVILLE|   TX|78028|         Corporation|     null|          12|  04/08/2020|Texas Hill Countr...|\n",
      "|  149942.0|   SAN ANTONIO|   TX|78211|Limited  Liabilit...|     null|           0|  04/29/2020|            BBVA USA|\n",
      "|  149939.0|CORPUS CHRISTI|   TX|78408|         Corporation|     null|          18|  04/28/2020|American Bank, Na...|\n",
      "|  149930.0|        AUSTIN|   TX|78757|Limited  Liabilit...|     null|          25|  05/03/2020|Bank of America, ...|\n",
      "|  149927.0|          KATY|   TX|77494|Subchapter S Corp...|     null|          19|  04/29/2020|    Bank of the West|\n",
      "|  149925.0|   HALTOM CITY|   TX|76117|         Corporation|     null|          15|  05/03/2020|    Cross River Bank|\n",
      "|  149915.0|        AUSTIN|   TX|78748|Limited  Liabilit...|     null|          13|  05/01/2020|JPMorgan Chase Ba...|\n",
      "|  149910.0|    FORT WORTH|   TX|76123|Non-Profit Organi...|        Y|          42|  04/07/2020|Community Nationa...|\n",
      "|  149900.0|       HOUSTON|   TX|77079|         Partnership|     null|           0|  04/28/2020|Capital One, Nati...|\n",
      "|  149900.0|   SAN ANTONIO|   TX|78260|Non-Profit Organi...|        Y|          27|  04/15/2020|          Frost Bank|\n",
      "|  149900.0|        DALLAS|   TX|75211|Limited  Liabilit...|     null|           0|  04/29/2020|Live Oak Banking ...|\n",
      "|  149900.0|       HOUSTON|   TX|77098|         Corporation|     null|          18|  04/11/2020|          Frost Bank|\n",
      "|  149900.0|        DENTON|   TX|76208|Limited  Liabilit...|     null|          12|  04/12/2020|    Independent Bank|\n",
      "|  149900.0|        AUSTIN|   TX|78758|Limited  Liabilit...|     null|          31|  04/04/2020|            BBVA USA|\n",
      "|  149900.0|        ATHENS|   TX|75751|Subchapter S Corp...|     null|           9|  04/13/2020|    First State Bank|\n",
      "|  149900.0|CORPUS CHRISTI|   TX|78408|Subchapter S Corp...|     null|           0|  04/30/2020|     Prosperity Bank|\n",
      "+----------+--------------+-----+-----+--------------------+---------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Retrieve Data from the New table \\n\")\n",
    "spark.sql(\"Select * from ValidationTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1db4a-e057-4d94-bf40-9277a160d3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
