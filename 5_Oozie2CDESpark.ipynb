{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b366ae-4160-4492-bf10-5c501a3a1204",
   "metadata": {},
   "source": [
    "## Migrating Oozie Spark Actions to Spark CDE Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68f67de-9a28-4891-8723-e57bf716ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import requests\n",
    "from requests_toolbelt import MultipartEncoder\n",
    "import xmltodict as xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64ed71b-a699-4735-a329-245e029406a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"WORKLOAD_USER\"] = \"pauldefusco\"\n",
    "os.environ[\"JOBS_API_URL\"] = \"https://tk5p4pn9.cde-6fr6l74r.go01-dem.ylcu-atmi.cloudera.site/dex/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefef9ba-92e1-4066-8c43-acbc0a350a88",
   "metadata": {},
   "source": [
    "#### We reuse the Python methods from Notebook 4 to construct requests to CDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c869d603-b658-4a4d-9a58-99468093abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user token to interact with CDE Service remotely\n",
    "def set_cde_token():\n",
    "    rep = os.environ[\"JOBS_API_URL\"].split(\"/\")[2].split(\".\")[0]\n",
    "    os.environ[\"GET_TOKEN_URL\"] = os.environ[\"JOBS_API_URL\"].replace(rep, \"service\").replace(\"dex/api/v1\", \"gateway/authtkn/knoxtoken/api/v1/token\")\n",
    "    token_json = !curl -u $WORKLOAD_USER:$WORKLOAD_PASSWORD $GET_TOKEN_URL\n",
    "    os.environ[\"ACCESS_TOKEN\"] = json.loads(token_json[5])[\"access_token\"]\n",
    "    \n",
    "    return json.loads(token_json[5])[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527d7b8-a243-4ce0-9c09-99a7480f558d",
   "metadata": {},
   "source": [
    "#### This time we upload one file at a time. The method is almost identical to \"put_files\" in notebook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417664f9-d99d-4e1d-a4c6-fca9d8726012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload Spark CDE Job file to CDE Resource\n",
    "def put_file(resource_name, job_path, tok):\n",
    "         \n",
    "    print(\"Working on Job: {}\".format(job_path.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "    m = MultipartEncoder(\n",
    "        fields={\n",
    "                'file': ('filename', open(job_path, 'rb'), 'text/plain')}\n",
    "        )\n",
    "\n",
    "    PUT = '{jobs_api_url}/resources/{resource_name}/{file_name}'.format(jobs_api_url=os.environ[\"JOBS_API_URL\"], \n",
    "                                                                                      resource_name=resource_name, \n",
    "                                                                                      file_name=job_path.split(\"/\")[-1])\n",
    "\n",
    "    x = requests.put(PUT, data=m, headers={'Authorization': f\"Bearer {tok}\",'Content-Type': m.content_type})\n",
    "\n",
    "    print(\"Response Status Code {}\".format(x.status_code))\n",
    "    print(x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373c4d8-c7de-4b41-9a90-fe492f2cbe4a",
   "metadata": {},
   "source": [
    "#### Similarly, we only create a Spark CDE Job at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5822234-0fda-4078-bea8-a2b98a3e8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_from_resource(job_path, tok, cde_payload):\n",
    "        \n",
    "    print(\"Working on Job: {}\".format(job_path.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "    headers = {\n",
    "    'Authorization': f\"Bearer {tok}\",\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "    PUT = '{}/jobs'.format(os.environ[\"JOBS_API_URL\"])\n",
    "    \n",
    "    data = json.dumps(cde_payload)\n",
    "\n",
    "    x = requests.post(PUT, headers=headers, data=data)\n",
    "\n",
    "    print(\"Response Status Code {}\".format(x.status_code))\n",
    "    print(x.text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ea882-ef06-4adb-a88c-67745404a35a",
   "metadata": {},
   "source": [
    "#### The Spark Oozie Action is an XML file. It can be easily converted into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228187f3-fb1a-4732-9bc3-453f8c043d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ooziexml_to_dict(ooziexml_path):\n",
    "\n",
    "    with open(ooziexml_path,'rb') as f:\n",
    "        d = xd.parse(f)\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd18de44-36ce-4a31-aaac-4f35fd1f0557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workflow-app': {'@xmlns': 'uri:oozie:workflow:0.5',\n",
       "  '@name': 'SparkWordCount',\n",
       "  'start': {'@to': 'spark-node'},\n",
       "  'action': {'@name': 'spark-node',\n",
       "   'spark': {'@xmlns': 'uri:oozie:spark-action:0.1',\n",
       "    'job-tracker': '${jobTracker}',\n",
       "    'name-node': '${nameNode}',\n",
       "    'prepare': {'delete': {'@path': '${nameNode}/user/${wf:user()}/${examplesRoot}/output-data'}},\n",
       "    'master': '${master}',\n",
       "    'name': 'SparkPi',\n",
       "    'class': 'org.apache.spark.examples.SparkPi',\n",
       "    'jar': 'example_spark_jobs/jobs/pi.scala',\n",
       "    'spark-opts': '--executor-memory 2G --num-executors 5',\n",
       "    'arg': 'value=10'},\n",
       "   'ok': {'@to': 'end'},\n",
       "   'error': {'@to': 'fail'}},\n",
       "  'kill': {'@name': 'fail',\n",
       "   'message': 'Workflow failed, error\\n            message[${wf:errorMessage(wf:lastErrorNode())}]'},\n",
       "  'end': {'@name': 'end'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ooziexml_to_dict('oozie_workflows/Workflow.xml')\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e674d8c-d543-4f36-a3bb-752436f1de99",
   "metadata": {},
   "source": [
    "#### This method is used to create Payloads for Spark CDE Jobs from the Spark Oozie Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e43d60-6855-4da1-8faa-42373b9df7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oozie_to_cde(cde_resource, d):\n",
    "    \n",
    "    cde_payload = { \"name\": \"job_name\", \n",
    "                \"type\": \"spark\", \n",
    "                \"retentionPolicy\": \"keep_indefinitely\", \n",
    "                \"mounts\": [ { \"dirPrefix\": \"/\", \"resourceName\": \"resource_name\" } ], \n",
    "                \"spark\": { \"file\": \"file_name\", \n",
    "                          \"conf\": { \"spark.pyspark.python\": \"python3\" }}, \n",
    "                \"schedule\": { \"enabled\": False}\n",
    "               }\n",
    "    \n",
    "    if \"spark\" in d[\"workflow-app\"][\"action\"].keys():\n",
    "\n",
    "        cde_payload[\"name\"] = d[\"workflow-app\"][\"action\"][\"spark\"][\"name\"]\n",
    "        cde_payload[\"mounts\"][0][\"resourceName\"] = cde_resource \n",
    "        cde_payload[\"spark\"][\"file\"] = d[\"workflow-app\"][\"action\"][\"spark\"][\"jar\"].split(\"/\")[-1]\n",
    "\n",
    "        if len(d[\"workflow-app\"][\"action\"][\"spark\"][\"spark-opts\"])>0:\n",
    "            opts = d[\"workflow-app\"][\"action\"][\"spark\"][\"spark-opts\"]\n",
    "            spark_job_opts = dict(np.array_split(opts.split(\" \"), len(opts.split(\" \"))/2))\n",
    "\n",
    "        if \"--driver-cores\" in spark_job_opts.keys():\n",
    "            cde_payload[\"spark\"][\"driverCores\"] = int(spark_job_opts[\"--driver-cores\"])\n",
    "\n",
    "        if \"--executor-cores\" in spark_job_opts.keys():\n",
    "            cde_payload[\"spark\"][\"executorCores\"] = int(spark_job_opts[\"--executor-cores\"])\n",
    "\n",
    "        if \"--driver-memory\" in spark_job_opts.keys():\n",
    "            cde_payload[\"spark\"][\"driverMemory\"] = spark_job_opts[\"--driver-memory\"]\n",
    "\n",
    "        if \"--executor-memory\" in spark_job_opts.keys():\n",
    "            cde_payload[\"spark\"][\"executorMemory\"] = spark_job_opts[\"--executor-memory\"]\n",
    "\n",
    "        if \"--num-executors\" in spark_job_opts.keys():\n",
    "            cde_payload[\"spark\"][\"numExecutors\"] = int(spark_job_opts[\"--num-executors\"])\n",
    "\n",
    "        #if \"class\" in d[\"workflow-app\"][\"action\"][\"spark\"].keys():\n",
    "        #    cde_payload[\"spark\"][\"className\"] = d[\"workflow-app\"][\"action\"][\"spark\"][\"class\"]\n",
    "        \n",
    "    else:\n",
    "        print(\"Error. This is not a Spark Oozie Action\")\n",
    "        \n",
    "    print(\"Working on Spark CDE Job: {}\".format(cde_payload[\"name\"]))\n",
    "    print(\"Converted Spark Oozie Action into Spark CDE Payload\")\n",
    "    return cde_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e607e4d6-9adc-49b4-9844-847376c9efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Spark CDE Job: SparkPi\n",
      "Converted Spark Oozie Action into Spark CDE Payload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'SparkPi',\n",
       " 'type': 'spark',\n",
       " 'retentionPolicy': 'keep_indefinitely',\n",
       " 'mounts': [{'dirPrefix': '/', 'resourceName': 'python2cde'}],\n",
       " 'spark': {'file': 'pi.scala',\n",
       "  'conf': {'spark.pyspark.python': 'python3'},\n",
       "  'executorMemory': '2G',\n",
       "  'numExecutors': 5},\n",
       " 'schedule': {'enabled': False}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cde_payload = oozie_to_cde(\"python2cde\", d)\n",
    "cde_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b0e14-a3ae-4bca-a7cb-fe612fbfe1d2",
   "metadata": {},
   "source": [
    "#### With the converted Payload we can upload to a CDE Resource and create a CDE Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886564d8-4f1e-4d87-9d6c-70f9a4434a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = set_cde_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5ca744-cd83-441b-921a-5d70da49bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Job: pi\n",
      "Response Status Code 201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "put_file(\"python2cde\", \"example_spark_jobs/jobs/pi.scala\", tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb83ab3c-847b-4fbd-95aa-d13e06b00e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Job: pi\n",
      "Response Status Code 201\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_job_from_resource(\"python2cde\", \"example_spark_jobs/jobs/pi.scala\", tok, cde_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a498a-6476-4217-989e-bd2136685726",
   "metadata": {},
   "source": [
    "#### Finally, navigate to the CDE Jobs Page and verify that the job has been created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75e017-34fd-4590-b183-23ffb9d54dee",
   "metadata": {},
   "source": [
    "![alt text](images/oozie2cde.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
