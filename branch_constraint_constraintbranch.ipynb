{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c991362-1990-4098-a820-8881cc82ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (current_date, date_sub, dayofmonth, month,\n",
    "                                   to_date, year)\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbe1a5b-87ee-41ae-ade9-afc6fd115ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Create a date range generator\n",
    "\n",
    "    Args:\n",
    "        start_date (str): start date\n",
    "        end_date (str): end date\n",
    "\n",
    "    Returns:\n",
    "        a generator for date range\n",
    "\n",
    "    \"\"\"\n",
    "    for n in range(int((end_date - start_date).days + 1)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a552717a-55c9-461e-b9f3-873ee5b64d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_date_filter(start_date, end_date):\n",
    "    \"\"\" Constructs a Spark Dataframe.filter() that will match the columns 'year','month','day' to the date range between the start_date (inclusive) and end_date arguments (inclusive)\n",
    "\n",
    "    Args:\n",
    "        start_date (str):  \"2021-6-1\"\n",
    "        end_date (str): \"2021-6-2\"\n",
    "\n",
    "    Returns:\n",
    "        filter (str)\n",
    "\n",
    "    Example:\n",
    "        start_date =  date(2021, 6, 1)\n",
    "        end_date =  date(2021, 6, 2)\n",
    "        returns '(year = 2021 and month = 06 and day = 01) or (year = 2021 and month = 06 and day = 02)'\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    filter = \"\"\n",
    "    # create a list of strings '(year == %Y and month == %m and day == %d)'\n",
    "    times = [single_date.strftime('(year == %Y and month == %m and day == %d)') for single_date in daterange(start_date, end_date)]\n",
    "    # join list\n",
    "    filter = \" or \".join(times)\n",
    "    return filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d27d8d-f873-4dcb-9dd8-6590f7eb5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_input_date_arg(date_arg):\n",
    "    \"\"\" Checks the date argument for special cases:\n",
    "     - 'TODAY' is replaced with the current date\n",
    "     - 'n_DAYS_AGO' is replaced the date n days ago\n",
    "\n",
    "     Args:\n",
    "         date_arg (str): ideally \"TODAY\" or \"n_DAYS_AGO\"\n",
    "\n",
    "     Return:\n",
    "         a date (str if no match, or date if it matches today's date or a relative date to today)\n",
    "    \"\"\"\n",
    "    # check if it's supposed to be today's date\n",
    "    if \"TODAY\".casefold() == str(date_arg).casefold():\n",
    "        return date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # check for a relative date to today\n",
    "    n_days_ago = r'(\\d+)_DAYS_AGO'\n",
    "    match = re.match(n_days_ago, str(date_arg))\n",
    "    if match:\n",
    "        days = match.groups(1)[0]\n",
    "        return datetime.strftime(date.today() - timedelta(int(days)), \"%Y-%m-%d\")\n",
    "\n",
    "    # no special matches, return the original date string\n",
    "    return date_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2ce085-8964-44c6-ac3e-9ae1193b2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_number():\n",
    "    if os.path.exists('version.yaml'):\n",
    "        version = open('version.yaml', 'r').read()\n",
    "        if (len(version.strip()) != 0):\n",
    "            return version\n",
    "    return 'No version defined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd8a802-16a9-42e9-964d-affd2e0ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set top level variables\n",
    "database_raw = \"raw\"\n",
    "database_optimise = \"optimise\"\n",
    "database_mart = \"mart\"\n",
    "table_branch = \"branch_0\"\n",
    "table_constraint = \"constraint_0\"\n",
    "table_constraintbranch = \"constraintbranch_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9480251-1f58-4efa-895b-d2034ae50ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n"
     ]
    }
   ],
   "source": [
    "# initiate spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"full_branch_constraint_constraintbranch\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\", \"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", \"s3a://go01-demo/\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719c9233-b3fd-4603-95e7-59ead3bf156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "logger = log4jLogger.LogManager.getLogger(__name__)\n",
    "logger.info(\"Job is running on \"+ get_version_number())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72614fd1-df12-4f98-a28c-428972024f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"prod\"\n",
    "bucket = \"s3a://go01-demo/tp-cdp-datalake-landing-%s\" % (env)\n",
    "project = \"datalake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a2bab6-a8a2-480a-9bf6-dcc80b565b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive configuration\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "spark.conf.set(\"spark.hive.mapred.supports.subdirectories\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e49604-e722-448c-a88e-f79371dd92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop temp view\n",
    "spark.catalog.dropTempView(\"%s_mkt_%s\" % (database_raw, table_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a8c6d0e-7a4d-45a7-8c41-1f44ed1a5446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 647bcee2-c420-4686-b9de-c87b46831039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create raw database with location\n",
    "spark.sql(\n",
    "    \"CREATE DATABASE IF NOT EXISTS %s LOCATION '%s/%s/%s/%s'\" % (database_raw, bucket, env, project, database_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c16518-71bc-4d7a-a5f7-72fcd44b4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop table\n",
    "spark.sql(\"DROP TABLE IF EXISTS %s.%s\" % (database_raw, table_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbd85ae3-9484-46e1-8e53-b3a4a092f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read last 3 day's of partitions from landing parquet files\n",
    "basePath = \"s3a://go01-demo/tp-cdp-datalake-landing-%s/%s/raw_mkt_mkt%s\" % (\n",
    "    env, database_raw, table_branch)\n",
    "inputPath = \"s3a://go01-demo/tp-cdp-datalake-landing-%s/%s/raw_mkt_mkt%s/year=*/month=*/day=*\" % (\n",
    "    env, database_raw, table_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a76bfd49-6c1b-43b7-ac7a-7fca15fc7e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/tp-cdp-datalake-landing-prod/raw/raw_mkt_mktbranch_0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55378847-cf94-4ef4-914c-361814c5faf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/tp-cdp-datalake-landing-prod/raw/raw_mkt_mktbranch_0/year=*/month=*/day=*'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff9cf2dc-0666-4068-97e4-46ab1c95fc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# temporary: setup test environment but sharing the same kinesis source as dev\n",
    "#share_dev_source = spark.conf.get(\"spark.driver.share.dev.source\", \"false\")\n",
    "#if share_dev_source and share_dev_source == \"true\":\n",
    "#    basePath = \"s3a://go01-demo/tp-cdp-datalake-landing-%s/%s/raw_mkt_mkt%s\" % (\n",
    "#        \"prod\", database_raw, table_branch)\n",
    "#    inputPath = \"s3a://go01-demo/tp-cdp-datalake-landing-%s/%s/raw_mkt_mkt%s/year=*/month=*/day=*\" % (\n",
    "#        \"prod\", database_raw, table_branch)\n",
    "\n",
    "inputDf_raw_branch = spark.read.options(\n",
    "    mergeSchema=\"true\", basePath=basePath, recursiveFileLookup=True, pathGlobFilter=\"*.parquet\").parquet(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938dae6-270e-4743-8702-f202fa9fdae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409bfdb1-2414-4bc0-82a7-9b12fcc16458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(table_name='MKTBRANCH', operation='UPDATE', timestamp='1627817907216', primarykey='5510.0', branchid='5510.0', csmmrid='598da924-30c7-4f26-b168-9c3ecd8135b8', branchname='WHI_T1.T1', branchtype='XF', currentnearopen='0', currentneardead='0', currentfaropen='0', currentfardead='0', currentnearmw='5.134', currentfarmw='-5.1', currentnearflow='7', currentfarflow='-7', currentlimit='73', currenttap='4.0', threewindingtype='0.0', createdby='holbrookr', createddate='1245035717014', modifiedby='mbl_fuse', modifieddate='1627817737643', auditosuser='mbl_fuse', auditmodule='JDBC Thin Client', audithost='172.30.18.206', discrepancytime='', discrepancyskip='0', discrepancybranchname='', year=2021, month=8, day=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDf_raw_branch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ec6e423-3bee-49a4-add2-433432f4b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstDay = '2021-08-01'\n",
    "lastDay = '2021-08-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02c56ad8-d7be-4a88-a427-f48dd2650ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date range for job\n",
    "#firstDay=interpret_input_date_arg(spark.conf.get(\"spark.driver.firstDay\", '2_DAYS_AGO'))      # firstDay=\"2021-11-05\"\n",
    "#lastDay=interpret_input_date_arg(spark.conf.get(\"spark.driver.lastDay\", 'TODAY'))             # lastDay=\"2021-11-07\"\n",
    "logger.info(f'Data date range (\"firstDay\" -> \"lastDay\"): \"{firstDay}\" -> \"{lastDay}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5970fcd6-7ecb-4f72-88e0-b4aa2bc7b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day\n",
      "2021-08-01\n",
      "Last day\n",
      "2021-08-03\n"
     ]
    }
   ],
   "source": [
    "print(\"First day\")\n",
    "print(firstDay)\n",
    "print(\"Last day\")\n",
    "print(lastDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c14db858-a82d-494b-8974-5a07d2b8ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDf_raw_branch \\\n",
    "    .filter(create_spark_date_filter(firstDay, lastDay)) \\\n",
    "    .createOrReplaceTempView(\"raw_mkt_%s\" % (table_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9b515e3-1f5c-4ca0-ad21-bc4835aaf8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create hive table with location\n",
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS %s.%s (\n",
    "    operation string,\n",
    "    timestamp bigint,\n",
    "    primarykey string,\n",
    "    branchid string,\n",
    "    branchname string,\n",
    "    branchtype string,\n",
    "    currentnearopen int,\n",
    "    currentneardead int,\n",
    "    currentfaropen int,\n",
    "    currentfardead int,\n",
    "    currentnearmw float,\n",
    "    currentfarmw float,\n",
    "    currentnearflow float,\n",
    "    currentfarflow float,\n",
    "    currentlimit float,\n",
    "    threewindingtype int,\n",
    "    createddate bigint,\n",
    "    modifieddate bigint\n",
    ")\n",
    "PARTITIONED BY (modified date)\n",
    "STORED AS PARQUET LOCATION '%s/%s/%s/%s/%s'\n",
    "\"\"\" % (database_raw, table_branch, bucket, env, project, database_raw, table_branch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e0c57b1-0f68-49b9-9425-27418dee9b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw.branch_0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_raw + \".\" + table_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8c1c05b-d98f-4368-bd5f-cd62f39cab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert into hive table\n",
    "spark.sql(\"\"\"\n",
    "insert\n",
    "into %s.%s\n",
    "partition(modified)\n",
    "from raw_mkt_%s\n",
    "select\n",
    "    operation,\n",
    "    timestamp,\n",
    "    primarykey,\n",
    "    branchid,\n",
    "    branchname,\n",
    "    branchtype,\n",
    "    currentnearopen,\n",
    "    currentneardead,\n",
    "    currentfaropen,\n",
    "    currentfardead,\n",
    "    currentnearmw,\n",
    "    currentfarmw,\n",
    "    currentnearflow,\n",
    "    currentfarflow,\n",
    "    currentlimit,\n",
    "    threewindingtype,\n",
    "    createddate,\n",
    "    modifieddate,\n",
    "    to_date(from_unixtime(cast(round(modifieddate/1000) as bigint))) as modified\n",
    "\"\"\" % (database_raw, table_branch, table_branch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3aaf5d-a3a4-4a07-8849-3acb90c3f241",
   "metadata": {},
   "source": [
    "optimise_branch_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "645c07cc-9e36-4e5d-aad4-dbdb70447a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create database with location\n",
    "spark.sql(\n",
    "    \"CREATE DATABASE IF NOT EXISTS %s LOCATION '%s/%s/%s/%s'\" % (database_optimise, bucket, env, project, database_optimise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc6532a3-2f7a-47f4-b9f4-7e826f501ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS %s.%s (\n",
    "    operation string,\n",
    "    timestamp bigint,\n",
    "    primarykey string,\n",
    "    branchid string,\n",
    "    branchname string,\n",
    "    branchtype string,\n",
    "    currentnearopen int,\n",
    "    currentneardead int,\n",
    "    currentfaropen int,\n",
    "    currentfardead int,\n",
    "    currentnearmw float,\n",
    "    currentfarmw float,\n",
    "    currentnearflow float,\n",
    "    currentfarflow float,\n",
    "    currentlimit float,\n",
    "    threewindingtype int,\n",
    "    createddate bigint,\n",
    "    modifieddate bigint\n",
    ")\n",
    "PARTITIONED BY (modified date)\n",
    "STORED AS PARQUET LOCATION '%s/%s/%s/%s/%s'\n",
    "\"\"\" % (database_optimise, table_branch, bucket, env, project, database_optimise, table_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f7cb6bf-8862-4916-abfe-ee39f4b100bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertDf_raw_branch = spark.sql(\"\"\"\n",
    "select\n",
    "    operation,\n",
    "    timestamp,\n",
    "    primarykey,\n",
    "    branchid,\n",
    "    branchname,\n",
    "    branchtype,\n",
    "    currentnearopen,\n",
    "    currentneardead,\n",
    "    currentfaropen,\n",
    "    currentfardead,\n",
    "    currentnearmw,\n",
    "    currentfarmw,\n",
    "    currentnearflow,\n",
    "    currentfarflow,\n",
    "    currentlimit,\n",
    "    threewindingtype,\n",
    "    createddate,\n",
    "    modifieddate,\n",
    "    modified\n",
    "from %s.%s\n",
    "where\n",
    "    (modified>=date_sub(to_date(\"%s\", \"yyyy-MM-dd\"), 1))\n",
    "\"\"\" % (database_raw, table_branch, firstDay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ffb5d39-2db3-4d57-b1b1-943902041fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "insertDf_raw_branch.write.insertInto(\"%s.%s\" % (database_optimise, table_branch), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f66b6-866b-4d4e-a071-b0412ff06c24",
   "metadata": {},
   "source": [
    "raw_constraintbranch_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b06adda9-67fd-4002-836e-cfabd38ccbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create temp table\n",
    "spark.catalog.dropTempView(\"%s_mkt_%s\" % (database_raw, table_constraint))\n",
    "\n",
    "# create database with location\n",
    "spark.sql(\n",
    "    \"CREATE DATABASE IF NOT EXISTS %s LOCATION '%s/%s/%s/%s'\" % (database_raw, bucket, env, project, database_raw))\n",
    "\n",
    "# drop table\n",
    "spark.sql(\"DROP TABLE IF EXISTS %s.%s\" % (database_raw, table_constraint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13697e5b-2875-430e-a929-2f72502a40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read last 3 day's of partitions from landing parquet files\n",
    "basePath = \"s3a://go01-demo/tp-cdp-datalake-landing-%s/%s/raw_mkt_mkt%s\" % (\n",
    "    env, database_raw, table_constraintbranch)\n",
    "inputPath = \"s3a://go01-demo/tp-cdp-datalake-landing-%s/%s/raw_mkt_mkt%s/year=*/month=*/day=*\" % (\n",
    "    env, database_raw, table_constraintbranch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "149220fa-d6f2-42ff-80c7-ec5722c57189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/tp-cdp-datalake-landing-prod/raw/raw_mkt_mktconstraintbranch_0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7489dad5-484c-4c83-8113-c79447aa6f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inputDf_raw_constraintbranch = spark.read.options(\n",
    "    mergeSchema=\"true\", basePath=basePath, recursiveFileLookup=True, pathGlobFilter=\"*.parquet\").parquet(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe0915-9ac5-4353-8b47-8feea4f1eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
